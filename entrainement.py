import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Dropout, Flatten
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import AllChem
import matplotlib.pyplot as plt
import joblib

class KDPredictor:
    def __init__(self, fingerprint_bits=2048, fingerprint_radius=2):
        self.fingerprint_bits = fingerprint_bits
        self.fingerprint_radius = fingerprint_radius
        self.solvent_encoder = LabelEncoder()
        self.composition_encoder = LabelEncoder()  # Encodage global principal
        self.kd_scaler = StandardScaler()
        self.model = None
        self.is_trained = False
        self.valid_combinations = {}
        self.solvent_composition_map = {}  # Pour la validation seulement
        
    def smiles_to_fingerprint(self, smiles):
        """Convertit un SMILES en fingerprint mol√©culaire"""
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                print(f"Attention: SMILES invalide: {smiles}")
                return np.zeros(self.fingerprint_bits)
            fingerprint = AllChem.GetMorganFingerprintAsBitVect(
                mol, self.fingerprint_radius, nBits=self.fingerprint_bits
            )
            return np.array(fingerprint)
        except Exception as e:
            print(f"Erreur avec SMILES {smiles}: {e}")
            return np.zeros(self.fingerprint_bits)
    
    def extract_valid_combinations(self, data):
        """Extrait les combinaisons solvant-composition valides du dataset"""
        valid_combinations = {}
        
        for solvent in data['Syst√®me de solvant'].unique():
            compositions = data[data['Syst√®me de solvant'] == solvent]['Composition'].unique()
            valid_combinations[solvent] = list(compositions)
        
        return valid_combinations
    
    def load_and_preprocess_data(self, csv_path, sep=';'):
        """Charge et pr√©traite les donn√©es depuis le fichier CSV"""
        # Chargement des donn√©es
        data = pd.read_csv(csv_path, sep=sep)
        print(f"Donn√©es charg√©es: {len(data)} entr√©es")
        print(f"Colonnes: {data.columns.tolist()}")
        
        # V√©rification des donn√©es manquantes
        print("\nV√©rification des donn√©es manquantes:")
        print(data.isnull().sum())
        
        # Nettoyage des donn√©es
        data = data.dropna()
        print(f"Donn√©es apr√®s nettoyage: {len(data)} entr√©es")
        
        # Extraction des combinaisons valides (pour la validation seulement)
        self.valid_combinations = self.extract_valid_combinations(data)
        print(f"\nüîç Combinaisons valides trouv√©es:")
        for solvent, compositions in self.valid_combinations.items():
            print(f"   {solvent}: {len(compositions)} compositions")
        
        # Conversion des SMILES en fingerprints
        print("\nConversion des SMILES en fingerprints...")
        X_smiles = np.array([self.smiles_to_fingerprint(smiles) for smiles in data['Smiles']])
        
        # Encodage des syst√®mes de solvant (GLOBAL)
        print("Encodage des syst√®mes de solvant...")
        X_solvent = self.solvent_encoder.fit_transform(data['Syst√®me de solvant'])
        
        # Encodage des compositions (GLOBAL - pour l'entra√Ænement)
        print("Encodage GLOBAL des compositions...")
        X_composition = self.composition_encoder.fit_transform(data['Composition'])
        
        # Cr√©ation du mapping pour la validation (s√©par√© de l'entra√Ænement)
        print("Cr√©ation du mapping de validation...")
        self.solvent_composition_map = {}
        for solvent in data['Syst√®me de solvant'].unique():
            compositions = data[data['Syst√®me de solvant'] == solvent]['Composition'].unique()
            # On utilise l'encodage GLOBAL pour mapper
            encoded_compositions = self.composition_encoder.transform(compositions)
            self.solvent_composition_map[solvent] = {
                'compositions': list(compositions),
                'encoded_compositions': list(encoded_compositions),
                'mapping': dict(zip(compositions, encoded_compositions))
            }
        
        # Pr√©paration de la target (KD)
        y_kd = data['KD'].values.reshape(-1, 1)
        y_kd_scaled = self.kd_scaler.fit_transform(y_kd).flatten()
        
        print(f"\nR√©sum√© du pr√©traitement:")
        print(f"- Fingerprints: {X_smiles.shape}")
        print(f"- Solvants uniques: {len(self.solvent_encoder.classes_)}")
        print(f"- Compositions globales uniques: {len(self.composition_encoder.classes_)}")
        print(f"- Plage des KD: {data['KD'].min():.3f} √† {data['KD'].max():.3f}")
        
        # Afficher le mapping des compositions par solvant
        print(f"\nüìä Mapping des compositions par solvant (pour validation):")
        for solvent, mapping_info in self.solvent_composition_map.items():
            print(f"   {solvent}: {mapping_info['mapping']}")
        
        return {
            'smiles': X_smiles,
            'solvent': X_solvent,
            'composition': X_composition  # Utilise l'encodage GLOBAL
        }, y_kd_scaled, data['KD'].values, data
    
    def build_model(self, n_solvents, n_compositions):
        """Construit le mod√®le de r√©seau de neurones"""
        # Dimensions
        fingerprint_dim = self.fingerprint_bits
        
        # Inputs
        smiles_input = Input(shape=(fingerprint_dim,), name='smiles')
        solvent_input = Input(shape=(1,), name='solvent')
        composition_input = Input(shape=(1,), name='composition')
        
        # Embedding pour les solvants
        solvent_embed = Embedding(n_solvents, 10)(solvent_input)
        solvent_embed = Flatten()(solvent_embed)
        
        # Embedding pour les compositions (GLOBAL)
        composition_embed = Embedding(n_compositions, 10)(composition_input)
        composition_embed = Flatten()(composition_embed)
        
        # Concatenation de toutes les features
        concatenated = Concatenate()([smiles_input, solvent_embed, composition_embed])
        
        # Couches cach√©es (adapt√©es √† la taille de votre dataset)
        x = Dense(512, activation='relu')(concatenated)
        x = Dropout(0.4)(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.2)(x)
        x = Dense(64, activation='relu')(x)
        
        # Couche de sortie
        output = Dense(1, activation='linear', name='kd_prediction')(x)
        
        # Construction du mod√®le
        model = Model(
            inputs=[smiles_input, solvent_input, composition_input],
            outputs=output
        )
        
        # Compilation
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae', 'mse']
        )
        
        return model

    def prepare_data_for_training(self, X, y, test_size=0.2, random_state=42):
        """Pr√©pare les donn√©es pour l'entra√Ænement avec un split coh√©rent"""
        n_samples = len(y)
        indices = np.arange(n_samples)
        
        # Split des indices
        train_idx, test_idx = train_test_split(
            indices, test_size=test_size, random_state=random_state
        )
        
        # Split des donn√©es en utilisant les m√™mes indices pour toutes les features
        X_train = {
            'smiles': X['smiles'][train_idx],
            'solvent': X['solvent'][train_idx],
            'composition': X['composition'][train_idx]
        }
        X_test = {
            'smiles': X['smiles'][test_idx],
            'solvent': X['solvent'][test_idx],
            'composition': X['composition'][test_idx]
        }
        
        y_train = y[train_idx]
        y_test = y[test_idx]
        
        return X_train, X_test, y_train, y_test
    
    def train(self, csv_path, test_size=0.2, random_state=42, epochs=100, batch_size=32):
        """Entra√Æne le mod√®le sur les donn√©es"""
        try:
            # Chargement et pr√©traitement des donn√©es
            X, y_scaled, y_original, original_data = self.load_and_preprocess_data(csv_path)
            
            # V√©rification du nombre d'√©chantillons
            n_samples = len(y_scaled)
            print(f"\nNombre total d'√©chantillons: {n_samples}")
            
            # Ajustement de la taille du test set
            if n_samples < 100:
                test_size = 0.1  # Plus de donn√©es pour l'entra√Ænement
                print(f"Dataset de taille moyenne, test_size ajust√© √†: {test_size}")
            
            # Utilisation de la m√©thode pour pr√©parer les donn√©es
            X_train, X_test, y_train, y_test = self.prepare_data_for_training(
                X, y_scaled, test_size=test_size, random_state=random_state
            )
            
            print(f"Split des donn√©es:")
            print(f"- Entra√Ænement: {len(y_train)} √©chantillons")
            print(f"- Test: {len(y_test)} √©chantillons")
            
            # Construction du mod√®le
            n_solvents = len(self.solvent_encoder.classes_)
            n_compositions = len(self.composition_encoder.classes_)
            
            print(f"üìä Dimensions du mod√®le:")
            print(f"- Solvants: {n_solvents}")
            print(f"- Compositions globales: {n_compositions}")
            
            self.model = self.build_model(n_solvents, n_compositions)
            
            print("\nArchitecture du mod√®le:")
            self.model.summary()
            
            # Ajustement de la batch_size
            actual_batch_size = min(batch_size, len(y_train))
            if actual_batch_size != batch_size:
                print(f"Batch_size ajust√© √†: {actual_batch_size}")
            
            # Callbacks
            callbacks = [
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=15,
                    restore_best_weights=True
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=8,
                    min_lr=1e-7
                )
            ]
            
            # Entra√Ænement
            print("\nD√©but de l'entra√Ænement...")
            history = self.model.fit(
                X_train, y_train,
                epochs=epochs,
                batch_size=actual_batch_size,
                validation_data=(X_test, y_test),
                callbacks=callbacks,
                verbose=1
            )
            
            # √âvaluation
            print("\n√âvaluation sur le jeu de test:")
            test_loss = self.model.evaluate(X_test, y_test, verbose=0)
            print(f"Loss: {test_loss[0]:.4f}, MAE: {test_loss[1]:.4f}")
            
            # Calcul de la MAE sur les donn√©es originales
            y_pred_scaled = self.model.predict(X_test, verbose=0)
            y_pred_original = self.kd_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_test_original = self.kd_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
            mae_original = np.mean(np.abs(y_pred_original - y_test_original))
            print(f"MAE sur KD original: {mae_original:.4f}")
            
            self.is_trained = True
            
            return history, X_test, y_test, y_original, original_data
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'entra√Ænement: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def save_model(self, filepath, data=None):
        """Sauvegarde le mod√®le, les preprocesseurs et les combinaisons valides"""
        if not self.is_trained:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant de √™tre sauvegard√©")
        
        # Sauvegarde du mod√®le Keras
        self.model.save(f'{filepath}_model.h5')
        
        # Sauvegarde des preprocesseurs
        joblib.dump({
            'solvent_encoder': self.solvent_encoder,
            'composition_encoder': self.composition_encoder,  # Encodage GLOBAL
            'kd_scaler': self.kd_scaler,
            'fingerprint_bits': self.fingerprint_bits,
            'fingerprint_radius': self.fingerprint_radius,
            'solvent_composition_map': self.solvent_composition_map  # Pour validation
        }, f'{filepath}_preprocessors.pkl')
        
        # Sauvegarde des combinaisons valides
        joblib.dump(self.valid_combinations, f'{filepath}_combinations.pkl')
        
        print(f"üíæ Mod√®le sauvegard√©: {filepath}_model.h5")
        print(f"üíæ Preprocesseurs sauvegard√©s: {filepath}_preprocessors.pkl")
        print(f"üíæ Combinaisons valides sauvegard√©es: {filepath}_combinations.pkl")
        print(f"üìä Solvants: {len(self.solvent_encoder.classes_)}")
        print(f"üìä Compositions globales: {len(self.composition_encoder.classes_)}")

def plot_training_history(history):
    """Visualise l'historique d'entra√Ænement"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # Plot loss
    ax1.plot(history.history['loss'], label='Train Loss')
    ax1.plot(history.history['val_loss'], label='Val Loss')
    ax1.set_title('Model Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    
    # Plot MAE
    ax2.plot(history.history['mae'], label='Train MAE')
    ax2.plot(history.history['val_mae'], label='Val MAE')
    ax2.set_title('Model MAE')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('MAE')
    ax2.legend()
    
    plt.tight_layout()
    plt.show()

# Exemple d'utilisation
if __name__ == "__main__":
    # Initialisation du pr√©dicteur
    predictor = KDPredictor()
    
    try:
        # Entra√Ænement du mod√®le
        print("üöÄ D√©marrage de l'entra√Ænement...")
        history, X_test, y_test, y_original, original_data = predictor.train(
            csv_path='data.csv',
            test_size=0.2,
            epochs=100,
            batch_size=32
        )
        
        # Visualisation de l'entra√Ænement
        plot_training_history(history)
        
        # Sauvegarde du mod√®le
        predictor.save_model('kd_predictor_model', original_data)
        
        print("\n‚úÖ Entra√Ænement termin√© avec succ√®s!")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de l'entra√Ænement: {e}")